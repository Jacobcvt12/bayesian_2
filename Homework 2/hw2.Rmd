---
title:  Homework 2
author: "[Jacob Carey](mailto:jcarey15@jhu.edu)"
date:   "`r Sys.Date()`"
output: pdf_document
---

```{r libraries, echo=FALSE}
knitr::opts_chunk$set(cache=TRUE)
knitr::opts_chunk$set(echo=FALSE)
knitr::opts_chunk$set(warning=FALSE)
library(mvtnorm)
library(ggplot2)
source("FEV.data")
set.seed(90210)
```

**Problem 1**  
```{r data}
x <- c(102.4, 103.2, 101.9, 103.0, 101.2, 100.7, 
       102.5, 103.1, 102.8, 102.3, 101.9, 101.4)

y <- c(99.6, 100.1, 100.2, 101.1, 99.8, 100.2, 
       101.0, 100.1, 100.7, 101.1, 101.3, 100.2)

z <- y - x
```

**a)**

```{r p1a}
qplot(x-mean(x), z, geom="point", 
      main="Temperature Before and After Aspirin")
```

There appears to be a roughly linear association between the
(centered) baseline temperature and the decrease in temperature.

**b)**
In class, we showed that $\sigma^2 | y \sim 
\text{Inv-Gamma}((n-k)/2, s^2/2)$  
where $s^2=\frac{1}{n-k}(y - X \hat{\beta})'(y-X\hat{\beta})$.
We have that $p(\beta|y) = \int p(\beta|\sigma^2, y)
p(\sigma^2|y)d\sigma^2$. Instead of finding an analytical solution
for the marginal posterior of $\beta$, we use simulations to 
characterize the density. We use the following conditional posterior
for sampling
$\beta | y, \sigma^2 \sim N(\hat{\beta}, V_{\beta}\sigma^2$).
```{r p1b}
# http://www.biostat.umn.edu/~brad/7440/Lecture5.pdf
X <- matrix(c(rep(1, 12), x - mean(x)), ncol=2)
y <- z

beta.hat <- solve(t(X) %*% X) %*% t(X) %*% y
V.beta <- solve(t(X) %*% X)

n <- nrow(X)
k <- ncol(X)

s.2 <- 1 / (n - k) * t(y - X %*% beta.hat) %*% (y - X %*% beta.hat)

S <- 5e3
PHI <- matrix(0, nrow=S, ncol=3)

for (s in 1:S) {
    prec.s <- rgamma(1, (n - k) / 2, s.2 / 2)
    beta.s <- rmvnorm(1, beta.hat, V.beta / prec.s)

    PHI[s, ] <- c(prec.s, beta.s)
}

qplot(PHI[, 2], xlab=expression(beta[0]), main="Posterior")
qplot(PHI[, 3], xlab=expression(beta[1]), main="Posterior")
```

**c)**
By assumption of the linear model, $Z_{13} \sim 
N(x_{13} \beta, \sigma^2 I)$. We can write the predictive
density for the child as 
$p(Z_{13}|Z_1, ..., Z_{12}) = \int p(Z_{13} | \beta, \sigma^2)
p(\beta, \sigma^2 | Z_1, ..., Z_{12}) d\beta d\sigma^2$. We adjust
the baseline temperature of the child by subtracting the mean of the
baseline temperature training data from the baseline. We sample from
the predictive distribution to find the predictive density.

```{r p1c}
PHI <- numeric(S)
x.tilde <- c(1, 100 - mean(x))

for (s in 1:S) {
    prec.s <- rgamma(1, (n - k) / 2, s.2 / 2)
    beta.s <- rmvnorm(1, beta.hat, V.beta / prec.s)
    y.tilde <- rnorm(1, x.tilde %*% t(beta.s), 1 / prec.s)

    PHI[s] <- y.tilde
}

qplot(PHI, xlab=expression("Z"[13]), main="Predictive Density")
```

To solve for $\text{Pr}(Z_{13}<0|z_1,x_1, ..., z_{12},x_{12}, x_{13})$,
we count the proportion of samples less than 0. We find that this 
proportion (or the probability of being less than 0) is 
`r mean(PHI < 0)`.

**d)**
Write the R code to fit the regression with g-prior and compute the Bayes factor in favor of an association with baseline temperature. (You can take g = n = 12.) Does your inference about the association between the baseline temperature and the effect of aspirin change?
```{r p1d-sample}
b.0 <- c(0, 0)
Sigma.0 <- diag(c(10, 2))
nu.0 <- 0.01
sigma.0.2 <- 1

g <- n
H.g <- g / (g + 1) * X %*% solve(t(X) %*% X) %*% t(X)
SSR.g <- t(y) %*% (diag(g) - H.g) %*% y
S <- 5e3

sigma.2 <- 1 / rgamma(S, (nu.0 + n) / 2,
                      (nu.0 * sigma.0.2 + SSR.g) / 2)
beta <- matrix(0, nrow=S, ncol=2)
m <- g / (g + 1) * beta.hat


for (s in 1:S) {
    V <- g / (g + 1) * sigma.2[s] * solve(t(X) %*% X)
    beta[s, ] <- rmvnorm(1, m, V)
}
```
```{r p1d-bf}
# how much the data favor model z.a over z.b

# design matrices
X.a <- as.matrix(X[, 1])
X.b <- X

# degrees of freedom
p.z.a <- ncol(X.a)
p.z.b <- ncol(X.b)

# beta hats
beta.a <- solve(t(X.a) %*% X.a) %*% t(X.a) %*% y
beta.b <- solve(t(X.b) %*% X.b) %*% t(X.b) %*% y

# s.2.n is the estimated residual variance under the 
# least squares estimate for model n
s.2.a <- t(y - X.a %*% beta.a) %*% (y - X.a %*% beta.a) / (n - p.z.a)
s.2.b <- t(y - X.b %*% beta.b) %*% (y - X.b %*% beta.b) / (n - p.z.b)

# SSR for each model
SSR.a <- t(y - X.a %*% beta.a) %*% (y - X.a %*% beta.a)
SSR.b <- t(y - X.b %*% beta.b) %*% (y - X.b %*% beta.b)

bayes.factor <- (1 + n) ^ ((p.z.b - p.z.a) / 2) * 
    sqrt(s.2.a / s.2.b) * 
    ((s.2.b + SSR.b) / (s.2.a + SSR.a)) ^ ((n + 1) / 2)

# http://www.cs.berkeley.edu/~jordan/courses/260-spring10/lectures/lecture13.pdf
# p.gamma <- n - 2
# R.2.gamma <- 1 - t(y - X %*% beta.hat) %*% (y - X %*% beta.hat) / 
#              (t(y - mean(y)) %*% (y - mean(y)))
# 
# b.f.n <- (1 + g) ^ ((n - 1 - p.gamma) / 2)
# b.f.d <- (1 + g * (1 - R.2.gamma)) ^ ((n - 1) / 2)
# bayes.factor <- b.f.n / b.f.d
```

We estimate the following densities for the $\beta$ coefficients using
the $g$-prior.

```{r p1d-plot}
qplot(beta[, 1], xlab=expression(beta[0]))  
qplot(beta[, 2], xlab=expression(beta[1]))
```

In these plots, we see that the posterior distribution for the baseline
temperature is strongly below zero. Addtionally, we calculate a 
bayes factor of `r bayes.factor`, indicating that the data
favor the model with the baseline temperature over the null model quite
strongly. Note that we used the **unit information prior** to calculate
bayes factor.

**2**

**a)**
```{r p2a}
## NAIVE MODEL

# prior parameters
b.0 <- c(3, 0, 0)
Sigma.0 <- diag(5, 3)
nu.0 <- 1
sigma.0.2 <- 1
n <- length(fev)

# organize data
# X <- matrix(c(rep(1, length(age)), smoker, age, smoker * age), ncol=4)
X <- matrix(c(rep(1, length(age)), smoker, age), ncol=3)
y <- fev

# Gibbs sampler
S <- 5e3
beta.post <- matrix(0, nrow=S, ncol=3)
sigma.2.post <- numeric(S)

# first guess for sigma.2
sigma.2 <- 1 / rgamma(1, nu.0 / 2, nu.0 * sigma.0.2 / 2)

for (s in 1:S) {
    # update beta
    V.b <- solve(solve(Sigma.0) + t(X) %*% X / sigma.2)
    E.b <- V.b %*% (solve(Sigma.0) %*% b.0 + t(X) %*% y / sigma.2)
    beta <- t(rmvnorm(1, E.b, V.b))

    # update sigma.2
    nu.n <- nu.0 + n
    SSR <- sum((y - X %*% beta) ^ 2)
    ss.n <- nu.0 * sigma.0.2 + SSR
    sigma.2 <- 1 / rgamma(1, nu.n / 2, ss.n / 2)

    # save results
    beta.post[s, ] <- beta
    sigma.2.post[s] <- sigma.2
}

naive.model <- list(beta=beta.post, sigma.2=sigma.2.post)

## INTERACTION MODEL

# prior parameters
b.0 <- c(3, 0, 0, 0)
Sigma.0 <- diag(5, 4)
nu.0 <- 1
sigma.0.2 <- 1
n <- length(fev)

# organize data
# X <- matrix(c(rep(1, length(age)), smoker, age, smoker * age), ncol=4)
X <- matrix(c(rep(1, length(age)), smoker, age, smoker * age), ncol=4)
y <- fev

# Gibbs sampler
S <- 5e3
beta.post <- matrix(0, nrow=S, ncol=4)
sigma.2.post <- numeric(S)

# first guess for sigma.2
sigma.2 <- 1 / rgamma(1, nu.0 / 2, nu.0 * sigma.0.2 / 2)

for (s in 1:S) {
    # update beta
    V.b <- solve(solve(Sigma.0) + t(X) %*% X / sigma.2)
    E.b <- V.b %*% (solve(Sigma.0) %*% b.0 + t(X) %*% y / sigma.2)
    beta <- t(rmvnorm(1, E.b, V.b))

    # update sigma.2
    nu.n <- nu.0 + n
    SSR <- sum((y - X %*% beta) ^ 2)
    ss.n <- nu.0 * sigma.0.2 + SSR
    sigma.2 <- 1 / rgamma(1, nu.n / 2, ss.n / 2)

    # save results
    beta.post[s, ] <- beta
    sigma.2.post[s] <- sigma.2
}

interaction.model <- list(beta=beta.post, sigma.2=sigma.2.post)
```

We used gibbs sampling to fit a linear model with an interaction term.
We extended the prior multivariate mean to be (3, 0, 0, 0) and the prior
multivariate variance covaraince matrix to be a diagonal matrix of
5's.

The posterior probability that the interaction term is negative is
`r mean(interaction.model$beta[, 4] < 0)` and that it is postive is
`r round(mean(interaction.model$beta[, 4] > 0), 2)`.

**b)**
Based on the estimates from the posterior distribution, I conclude that 
there exists an interaction between smoking and age, and that this
interaction is negative. In other words, we expect for the gain in
FEV to be less by age for smokers.

**c)**
Slide 24 of the Lecture05 file gives the analytic formula for the Bayes factor in nested normal regression models. Compute the Bayes factor that compares the model with interaction to the model with just the main effects but without the interaction. What is the evidence in favor of the interaction based on the Bayes factor?
